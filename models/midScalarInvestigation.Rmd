---
title: "responseToFrankeTypicalUse..."
author: "Ben Peloquin"
date: "December 14, 2015"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_depth: 3
---
```{r}
rm(list = ls())
setwd("/Users/benpeloquin/Desktop/Projects/scalar_implicature/models")
library(tidyr)
source("../analysis/useful_dplyr.R")
library(dplyr)
library(rjson)
library(ggplot2)
library(MASS)
library(jsonlite)
```

```{r multiplot(), echo=FALSE}
multiplot = function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

# "neutral" target scalars for e12/e13

We're trying to come up with scalars that will approximate the blue distributions here:
["https://htmlpreview.github.io/?https://raw.githubusercontent.com/langcog/scalar_implicature/master/models/Model_additionalAlts.html#plot-distr"]

## Currently proposing for e12 / e13:

I've populated e12/e13 with the first items (1) listed below. So we're including `okay` with good_excellent because that was the most frequent and "applicable" alternative.

"Somebody said they thought the food was \___."

good_excellent scale: 1) `okay`, 2) `average`, 3) `fine`

palatable_delicious scale: 1) `mediocre`, 2) `okay`

memorable_unforgettable scale: 1) `ordinary`, 2) `average`, 3) `unremarkable`

"Somebody said they \___ the food."

liked_loved scale: 1) `felt indifferent about`, 2) `appreciated`

(we might want to go with something like "felt okay about" or "felt fine about", but those weren't top competitors for liked_loved)

"Somebody said they enjoyed \___ of the food."

some_all scale: 1) `a little bit of`, 2) `bits of` 3) `parts of`

# Study e9 - Empirically derived alts (frequencies)
```{r}
d = fromJSON("../analysis/edited_alts.json", simplifyDataFrame=T)
d = as.data.frame(d, stringsAsFactors = FALSE)

d = d %>% 
  gather(base, alt, 
         palatable, liked, good, loved, some, all, 
         delicious, excellent, memorable, unforgettable) %>%
  mutate(base = as.character(base)) %>%
  rowwise %>%
  mutate(scale = ifelse(base == "some" | base == "all", "some_all", 
                        ifelse(base == "palatable" | base == "delicious", "palatable_delicious",
                               ifelse(base == "liked" | base == "loved", "liked_loved",
                                      ifelse(base == "good" | base == "excellent", 
                                             "good_excellent", "memorable_unforgettable"))))) %>%
  group_by(scale, alt) %>%
  summarise(n = n())

# Plot findings of salient alternatives for each scale
# -------------------------------------------------------
scales = unique(d$scale)
ggplot(data=d[d$scale==scales[1],], aes(x=reorder(alt, -n), y=n)) +
  geom_bar(stat="identity") + ylim(0, 60) + labs(x = "alternative", y = "counts") +
  ggtitle(paste("Salient alternatives for ", scales[1])) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(data=d[d$scale==scales[2],], aes(x=reorder(alt, -n), y=n)) +
  geom_bar(stat="identity") + ylim(0, 60) + labs(x = "alternative", y = "counts") +
  ggtitle(paste("Salient alternatives for ", scales[2])) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(data=d[d$scale==scales[3],], aes(x=reorder(alt, -n), y=n)) +
  geom_bar(stat="identity") + ylim(0, 60) + labs(x = "alternative", y = "counts") +
  ggtitle(paste("Salient alternatives for ", scales[3])) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(data=d[d$scale==scales[4],], aes(x=reorder(alt, -n), y=n)) +
  geom_bar(stat="identity") + ylim(0, 60) + labs(x = "alternative", y = "counts") +
  ggtitle(paste("Salient alternatives for ", scales[4])) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
ggplot(data=d[d$scale==scales[5],], aes(x=reorder(alt, -n), y=n)) +
  geom_bar(stat="identity") + ylim(0, 60) + labs(x = "alternative", y = "counts") +
  ggtitle(paste("Salient alternatives for ", scales[5])) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# multiplot(p1, p2, p3, p4, p5, cols=1)
```

# Emp alts for Some / All individually
```{r}
d = fromJSON("../analysis/edited_alts.json", simplifyDataFrame=T)
d = as.data.frame(d, stringsAsFactors = FALSE)

d.some = d$some
d.some = as.data.frame(as.factor(d.some))
d.some = as.data.frame(table(d.some))
d.some = d.some[rev(order(d.some$Freq)), ]
d.some$prop = d.some$Freq / sum(d.some$Freq)
ggplot(d.some, aes(x=reorder(d.some, Freq), y=Freq)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("alternatives") +
  ylab("frequency counts") +
  ggtitle("e9 - alternatives to 'some'")


d.all = as.data.frame(as.factor(d.all))
d.all = as.data.frame(table(d.all))
d.all = d.all[rev(order(d.all$Freq)), ]
d.all$prop = d.all$Freq / sum(d.all$Freq)
ggplot(d.all, aes(x=reorder(d.all, Freq), y=Freq)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("alternatives") +
  ylab("frequency counts") +
  ggtitle("e9 - alternatives to 'all'")

d.good = d$good
d.good = as.data.frame(as.factor(d.good))
d.good = as.data.frame(table(d.good))
d.good = d.good[rev(order(d.good$Freq)), ]
d.good$prop = d.good$Freq / sum(d.good$Freq)
ggplot(d.good, aes(x=reorder(d.good, Freq), y=Freq)) +
  geom_bar(stat="identity", position="dodge") +
  xlab("alternatives") +
  ylab("frequency counts") +
  ggtitle("e9 - alternatives to 'good'")
# bad
d.good[d.good$d.good == "bad", "prop"] / sum(d.good$prop)
# terrrible
d.good[d.good$d.good == "terrible", "prop"] / sum(d.good$prop)
salience <- c(1, 1, 0.05, 0.08, 0.22)
normed.salience <- salience / sum(salience)

top.4 <- data.frame(c("excellent", "good", "okay", "bad", "terrible"),
                    c(1, 1, 0.05, 0.22))
```

```{r}
d <- fromJSON("../analysis/edited_alts.json", simplifyDataFrame=T) %>% as.data.frame(., stingasFactors = FALSE)

scales <- c("good_excellent",
            "liked_loved",
            "palatable_delicious",
            "memorable_unforgettable",
            "some_all")
degrees <- c("hi1", "hi2", "mid", "low1", "low2")

# alternative counts
altCounts <- data.frame(Var1 = c(), Freq = c(), percent = c())
for (s in scales) {
  scaleHeads <- strsplit(s, "_")[[1]]
  s1 <- scaleHeads[1]
  s2 <- scaleHeads[2]
  combinedDf <- rbind(as.data.frame(table(d[, s1])), as.data.frame(table(d[, s2]))) %>%
    aggregate(Freq ~ Var1 ,data = ., FUN = sum) %>%
    mutate(scale = s,
           percent = Freq / sum(Freq))
  altCounts <- rbind(altCounts, combinedDf)
}
fullScales <- c("excellent", "good", "okay", "bad", "terrible",
               "loved", "liked", "indifferent", "disliked", "hated",
               "delicious", "palatable", "mediocre", "gross", "disgusting",
               "unforgettable", "memorable", "ordinary", "bland", "forgettable",
               "all", "most", "some", "little", "none")
topAlts <- data.frame(good_excellent = c("okay", "bad", "terrible"),
             liked_loved = c("indifferent", "disliked", "hated"),
             palatable_delicious = c("mediocre", "gross", "disgusting"),
             memorable_unforgettable = c("ordinary", "bland", "forgettable"),
             some_all = c("most", "little", "none"))
scales
saliences <- data.frame(scale = unlist(sapply(scales, FUN=function(i){rep(i, 5)})),
                        degree = unlist(sapply(degrees, FUN=function(i){rep(i, 5)})),
                        salienceCost = rep(NA, 5^2))

saliences <- data.frame(scale = c(rep(scales[1], 5),
                                  rep(scales[2], 5),
                                  rep(scales[3], 5),
                                  rep(scales[4], 5),
                                  rep(scales[5], 5)),
                        degree = rep(degrees, 5),
#                                   rep(degrees[2], 5),
#                                   rep(degrees[3], 5),
#                                   rep(degrees[4], 5),
#                                   rep(degrees[5], 5)),
                        salienceCost = rep(NA, 5^2),
                        stringsAsFactors = FALSE)

# subset(altCounts, scale == "good_excellent" & Var1 == "amazing")$percent
# mapply(saliences$scale, saliences$degree, FUN = function(s, d) {
#   subset(altCounts, scale == s & degree == d)$percent
# })

# Saliency

# good_excellent
saliences[saliences$scale == "good_excellent",]$salienceCost <- c(1, 1, 0.022, 0.189, 0.089)
saliences[saliences$scale == "liked_loved",]$salienceCost <- c(1, 1, 0.0056, 0.25, 0.28)
saliences[saliences$scale == "palatable_delicious",]$salienceCost <- c(1, 1, 0.011, 0.089, 0.133)
saliences[saliences$scale == "memorable_unforgettable",]$salienceCost <- c(1, 1, 0.0389, 0.0722, 0.022)
saliences[saliences$scale == "some_all",]$salienceCost <- c(1, 0.1278, 1, 0.09444, 0.2167)
write.csv(saliences, file="saliences.csv")

currScale = "some_all"
subset(altCounts, scale == currScale & Var1 %in% topAlts[, currScale])
```

