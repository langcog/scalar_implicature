---
title: "Model_additionalAlts"
author: "BPeloquin"
date: "November 25, 2015"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_depth: 3
---
```{r include=FALSE}
rm(list=ls())

library(ggplot2)
library(knitr)
library(rjson)
library(dplyr)
library(tidyr)
library(gridExtra)
```

# RSA functionality
```{r}
# speaker informativity
# ---------------------
speaker.inf = function(d, alpha, cost = 0) {
  exp(alpha*(log(d) - cost))
}
# speaker likelihood
# ------------------
speaker.lhd = function(rating, degree, m, alpha) {
  numerator = speaker.inf(m[rating, degree], alpha)
  normalize = sum(sapply(m[rating, ], function(i) {speaker.inf(i, alpha)}))
  return(numerator / normalize)
}
# non-normalized posterior
# -----------------------
nn.post = function(rating, degree, m, alpha, useprior) {
  prior = priors[rating, "prior.p"]
  return(speaker.lhd(rating, degree, m, alpha) * prior)
}
# normalized posterior
# --------------------
norm.post = function(rating, degree, m, alpha, useprior) {
  nn = nn.post(rating, degree, m, alpha, useprior)
  normalize = sum(unlist(sapply(seq(1, 5), function(i){nn.post(i, degree, m, alpha, useprior)})))
  return(nn / normalize)
}
```

# Run model functionality
```{r}
# run.partial()
# ------------
# Run RSA with model1 (entailment) and model2 (entailment + generic)
run.partial = function(d, alpha=1, useprior=F, usenone=F, normalize=F) {
  if (normalize) {
    mat = d %>%
      select(stars, degree, speaker.p) %>%
      spread(degree, speaker.p) %>%
      mutate(hi = hi / sum(hi),
             low = low / sum(low)) %>%
      select(hi, low)
  } else {
    mat = d %>%
      select(stars, degree, speaker.p) %>%
      spread(degree, speaker.p) %>%
      select(hi, low)
  }
  
  if (usenone) {
    mat$none = c(1, 0, 0, 0, 0)
  } 
  
  d$pred = round(as.numeric(mapply(norm.post, d$stars, d$degree, 
                                    MoreArgs = list(m = mat, 
                                                    alpha = alpha, 
                                                    useprior = useprior))), 
                  digits=4)
  
  return(d)
}

# run.full()
# ------------
# Run RSA with model3 (full) with alternatives
run.full = function(d, alpha=1, useprior=F, usenone=F, addMid=F, normalize=F) {
  if (addMid) {
    if (normalize) {
      mat = d %>%
        select(stars, degree, speaker.p) %>%
        spread(degree, speaker.p) %>%
        mutate(hi1 = hi1 / sum(hi1),
               hi2 = hi2 / sum(hi2),
               mid = mid / sum(mid),
               low1 = low1 / sum(low1),
               low2 = low2 / sum(low2)) %>%
        select(hi1, hi2, mid, low1, low2)    
    } else {
      mat = d %>%
        select(stars, degree, speaker.p) %>%
        spread(degree, speaker.p) %>%
        select(hi1, hi2, mid, low1, low2)    
    }
  } else {
    if (normalize) {
      mat = d %>%
        select(stars, degree, speaker.p) %>%
        spread(degree, speaker.p) %>%
        mutate(hi1 = hi1 / sum(hi1),
               hi2 = hi2 / sum(hi2),
               low1 = low1 / sum(low1),
               low2 = low2 / sum(low2)) %>%
        select(hi1, hi2, low1, low2)
    } else {
      mat = d %>%
        select(stars, degree, speaker.p) %>%
        spread(degree, speaker.p) %>%
        select(hi1, hi2, low1, low2)
    }
  }
  
  if (usenone) {
    mat$none = c(1, 0, 0, 0, 0)
  } 
  
  d$pred = round(as.numeric(mapply(norm.post, d$stars, d$degree, 
                                    MoreArgs = list(m = mat, 
                                                    alpha = alpha, 
                                                    useprior = useprior))), 
                  digits=4)
  
  return(d)
}
```

## Tune hyperparams
```{r}
# tune.alhpa()
# ------------
# d        --> data
# alphas   --> range of alphas to test
# type     --> full or partial model
# useprior --> use uniform prior
# usenone  --> use gener None
tune.alpha = function(d, alphas = seq(from=1, to=10),
                      type="partial", useprior = T,
                      usenone=F, compare.data=NULL, addMid = F, normalize=F) {
  # Tune best alphas
  fit = sapply(alphas, FUN=function(n) {
    if (type == "partial") {
      md = d %>%
        do(run.partial(., alpha=n, useprior=useprior, usenone=usenone, normalize=normalize))
    } else {
      md = d %>%
        do(run.full(., alpha=n, useprior=useprior, usenone=usenone, addMid=addMid, normalize=normalize))
    }
    # Fit to e6 data
    if (!is.null(compare.data)) {
      matched.items = which((md[, "scale"] != "some_all" &
                               (md[, "degree"] == "hi2" | md[, "degree"] == "hi1")) |
                              (md[, "scale"] == "some_all" &
                                 (md[, "degree"] == "hi1" | md[, "degree"] == "low1")))
      md = md[matched.items, ]
      md$degree = ifelse(md$degree == "hi1", "hi", "low")
      stopifnot("listener.p" %in% colnames(compare.data))
      md$listener.p = compare.data$listener.p
    }
    
    # MSE
    return(mean((md$pred - md$listener.p)^2))
  })  
  # get lowest MSE
  best.alpha = which(fit == min(fit))
  return(best.alpha)
}

# Uniform priors
unif.priors = data.frame(stars = seq(1, 5), prior.p = rep(0.2, 5))
emp.priors = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/emp_priors.csv")
priors = unif.priors

scales.entropy = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/scales_entropy.csv")
```

# Model Comparisons

### Data for entailment models
```{r entailment_models_data, message=FALSE, warning=FALSE}
speaker = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L0_e8.csv")
listener = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L1_e6.csv")
# Combine speaker / listener
data.partial = left_join(speaker, listener) %>%
  left_join(priors) %>%
  rowwise %>%
  select(scale, degree, stars, speaker.p, listener.p, prior.p) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p)) %>%
  group_by(scale)
```

### Data for full model - no alternatives
```{r full_model_data, message=FALSE, warning=FALSE}
speaker = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L0_e10.csv")
listener = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L1_e11.csv")
data.full = left_join(speaker, listener) %>%
  left_join(priors) %>%
  rowwise %>%
  select(scale, degree, stars, speaker.p, listener.p, prior.p) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p)) %>%
  group_by(scale)

data.full$entropy = scales.entropy[data.full$scale, ]$Entropy
```

### Data for full model - alternatives
```{r}
speaker = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L0_e10a.csv")
data.full.extras = left_join(speaker, listener) %>%
  left_join(priors) %>%
  rowwise %>%
  select(scale, degree, stars, speaker.p, listener.p, prior.p) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p)) %>%
  group_by(scale)
```

## Match items between studies (full and partial)
```{r}
matched.items = which((data.full[, "scale"] != "some_all" &
        (data.full[, "degree"] == "hi2" | data.full[, "degree"] == "hi1")) |
       (data.full[, "scale"] == "some_all" &
          (data.full[, "degree"] == "hi1" | data.full[, "degree"] == "low1")))
matched.items.extras = which((data.full.extras[, "scale"] != "some_all" &
        (data.full.extras[, "degree"] == "hi2" | data.full.extras[, "degree"] == "hi1")) |
       (data.full.extras[, "scale"] == "some_all" &
          (data.full.extras[, "degree"] == "hi1" | data.full.extras[, "degree"] == "low1")))
```

Focus on single scalar fn()
```{r}
# focus_alts
# ---------
# target    --> specific scalar family we're interested in (i.e. "some_all")
# combos    --> collection of possible 'mid' l0 distribution
# normalize --> normaize pre RSA?
# alpha     --> set alpha level
focus_alts = function(target, combos, normalize=F, alpha=1) {
  # Default normalish looking distr
  if (is.null(combos)) {
    combos = data.frame(c(0, 0, 1, 0, 0),
                        c(0, 0.05, 0.9, 0.05, 0),
                        c(0, 0.1, 0.8, 0.1, 0),
                        c(0, 0.2, 0.6, 0.2, 0),
                        c(0, 0.25, 0.5, 0.25, 0))  
  }
  colnames(combos) = LETTERS[1:length(colnames(combos))]
  
  # Find best distribution
  cors = c()
  for (i in 1:ncol(combos)) {
    ge.subset = data.full.extras[data.full.extras$scale == target, ]
    ge.subset[ge.subset$degree == "mid", "speaker.p"] = combos[, i]
    target.scalars = c("hi1", "hi2")
    
    # Run model
    m1 = ge.subset %>%
      do(run.full(., alpha = alpha, addMid=T, normalize=normalize))
    if (target == "some_all") {
      target.scalars = c("hi1", "low1")
      m1 = m1[m1$degree %in% target.scalars, ]  
    } else {
      m1 = m1[m1$degree %in% target.scalars, ]  
    }
    cors[i] = cor(m1$pred, m1$listener.p)  
  }
  
  # Get index of highest correlation
  i = which(cors == max(cors))[1]
  ge.subset = data.full.extras[data.full.extras$scale == target, ]
    ge.subset[ge.subset$degree == "mid", "speaker.p"] = combos[, i]
  m1 = ge.subset %>%
    do(run.full(., alpha = alpha, addMid=T, normalize=normalize))
  
  p = qplot(stars, listener.p, col=degree, 
        data=m1,
        ylab="Posterior p(rating | word)") + 
    geom_line(aes(y = pred), lty = 4)
  
  return(c(combos[i], cors[i]))
}
```

```{r}




# liked_loved
combos = data.frame(c(0, 0, 1, 0, 0),
                    c(0, 0.05, 1, .05, 0),
                    c(0, 0.1, 1, .1, 0),
                    c(0, 0.1, 0.9, .1, 0),
                    c(0, 0.2, 0.8, .2, 0),
                    c(0, 0.25, 0.8, .25, 0),
                    c(0.05, 0.1, 1, .1, 0.9),
                    c(0.1, 0.1, 1, .1, 0.1))  
focus_alts("liked_loved", combos=combos, alpha=4, normalize=T)
focus_alts("liked_loved", combos=combos, alpha=8, normalize=F)

# good_excellent
combos = data.frame(c(0, 0, 1, 0, 0),
                    c(0, 0.05, 0.9, 0.05, 0),
                    c(0, 0.1, 0.8, 0.1, 0),
                    c(0, 0.2, 0.2, 0.2, 0.9),
                    c(0, 0.2, 0.6, 0.2, 1),
                    c(0, 0.2, 0.6, 0.3, 0.2),
                    c(0, 0.2, 0.6, 0.3, 0.3),
                    c(0, 0.2, 0.6, 0.5, 0.4),
                    c(0, 0.2, 0.6, 0.5, 0.5),
                    c(0, 0.2, 0.6, 0.6, 0.4),
                    c(0.05, 0.2, 0.6, 0.2, 0.05),
                    c(0, 0.25, 0.5, 0.25, 0))  
focus_alts("good_excellent", combos=combos, alpha = 1, normalize=T)
focus_alts("good_excellent", combos=combos, alpha = 8, normalize=F)

# some_all
combos = data.frame(c(0, 1, 0, 0, 0),
                    c(0.05, 2, 0.05, 0.05, 0.05),
                    c(0.2, 0.9, 0.2, 0.1, 0.1),
                    c(0.2, 0.9, 0.2, 0.1, 0),
                    c(0.1, 0.9, 0.1, 0, 0),
                    c(0.1, 0.8, 0.1, 0, 0),
                    c(0.2, 0.6, 0.2, 0, 0),
                    c(0.25, 0.5, 0.25, 0, 0))
focus_alts("some_all", combos=combos, alpha=4, normalize=T)
focus_alts("some_all", combos=combos, alpha=8, normalize=F)

# memorable_unforgettable
combos = data.frame(rep(0.1, 5),
                    c(0.05, 2, 0.05, 0.05, 0.05),
                    c(0.05, 1, 0.05, 0.05, 0.05),
                    c(0.05, 0.9, 0.05, 0.05, 0.05),
                    c(0.05, 0.8, 0.05, 0.05, 0.05),
                    c(0.05, 0.7, 0.05, 0.05, 0.05),
                    c(0.05, 0.6, 0.05, 0.05, 0.05),
                    c(0.05, 0.5, 0.05, 0.05, 0.05),
                    c(0.1, 0.8, 0.1, 0.1, 0.1))
focus_alts("memorable_unforgettable", combos=combos, alpha=4, normalize=T)
focus_alts("memorable_unforgettable", combos=combos, alpha=8, normalize=F)

# palatable_delicious
combos = data.frame(rep(0.1, 5),
                    c(0.05, 2, 0.05, 0.05, 0.05),
                    c(0.05, 1, 0.05, 0.05, 0.05),
                    c(0.05, 0.05, 0.05, 0.9, 0.1),
                    c(0.05, 0.05, 0.05, 0.9, 0.05),
                    c(0.05, 0.7, 0.05, 0.5, 0.05),
                    c(0.05, 0.6, 0.05, 0.05, 0.05),
                    c(0.05, 0.5, 0.05, 0.05, 0.05),
                    c(0.1, 0.8, 0.1, 0.1, 0.1))
focus_alts("palatable_delicious", combos=combos, alpha=4, normalize=T)
focus_alts("palatable_delicious", combos=combos, alpha=8, normalize=F)
```

Fit params - weight update fn()
```{r}
# fit_distr()
# -----------
# target        --> specific scalar family we're interested in (i.e. "some_all")
# numIters      --> number of iterations to run
# eta           --> step size
# alpha         --> alpha level for RSA?
# normalize     --> normalize in RSA?
fit_distr = function(target, numIters = 100, eta=0.01, normalize=T, alpha=1, verbose=F) {
  # Initialize to uniform
  w = rep(0.1, 5)
  # Focus on scalar 'target'
  scalar.focused = data.full.extras[data.full.extras$scale == target, ]
  
  # For numIters iterations...
  for (i in 1:numIters) {
    scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = w
    m = scalar.focused %>%
      do(run.full(., alpha = alpha, addMid=T, normalize=normalize))
    w[1] = w[1] - eta * (sum(m[m$stars == 1, ]$listener.p - m[m$stars == 1, ]$pred))
    w[2] = w[2] - eta * (sum(m[m$stars == 2, ]$listener.p - m[m$stars == 2, ]$pred))
    w[3] = w[3] - eta * (sum(m[m$stars == 3, ]$listener.p - m[m$stars == 3, ]$pred))
    w[4] = w[4] - eta * (sum(m[m$stars == 4, ]$listener.p - m[m$stars == 4, ]$pred))
    w[5] = w[5] - eta * (sum(m[m$stars == 5, ]$listener.p - m[m$stars == 5, ]$pred))
    w = ifelse(w <= 0, 0, w)
    w = ifelse(w >= 1, 1, w)
    if(verbose) {print(w)}
  }
  return(w)
}
normalize = function(w) { return (w / sum(w))}

```


```{r}
targets = c("hi1", "hi2")
fitted.w = fit_distr("good_excellent", numIters=100, alpha=4, eta=0.001, normalize=T)
scalar.focused = data.full.extras[data.full.extras$scale == "good_excellent", ]
scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = fitted.w
m = scalar.focused %>%
  do(run.full(., alpha = 4, addMid=T, normalize=T))
cor(m[m$degree %in% targets, ]$listener.p, m[m$degree %in% targets, ]$pred)
normalize(fitted.w)


fitted.w = fit_distr("liked_loved", numIters=100, alpha=4, eta=0.001, normalize=T)
# fitted.w = fitted.w / sum(fitted.w)
scalar.focused = data.full.extras[data.full.extras$scale == "good_excellent", ]
scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = fitted.w
m = scalar.focused %>%
  do(run.full(., alpha = 4, addMid=T, normalize=T))
cor(m[m$degree %in% targets, ]$listener.p, m[m$degree %in% targets, ]$pred)
normalize(fitted.w)

targets = c("hi1", "low1")
fitted.w = fit_distr("some_all", numIters=200, alpha=4, eta=0.001, normalize=T)
# fitted.w = fitted.w / sum(fitted.w)
scalar.focused = data.full.extras[data.full.extras$scale == "some_all", ]
scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = fitted.w
m = scalar.focused %>%
  do(run.full(., alpha = 4, addMid=T, normalize=T))
cor(m[m$degree %in% targets, ]$listener.p, m[m$degree %in% targets, ]$pred)
normalize(fitted.w)
qplot(stars, listener.p, col=degree, 
      data=m[m$degree %in% targets,],
      ylab="Posterior p(rating | word)") + 
  ylim(0, 1) +
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)


targets = c("hi1", "hi2")
fitted.w = fit_distr("palatable_delicious", numIters=200, alpha=4, eta=0.001, normalize=T)
# fitted.w = fitted.w / sum(fitted.w)
scalar.focused = data.full.extras[data.full.extras$scale == "palatable_delicious", ]
scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = fitted.w
m = scalar.focused %>%
  do(run.full(., alpha = 4, addMid=T, normalize=T))
cor(m[m$degree %in% targets, ]$listener.p, m[m$degree %in% targets, ]$pred)
normalize(fitted.w)

qplot(stars, listener.p, col=degree, 
      data=m[m$degree %in% targets,],
      ylab="Posterior p(rating | word)") + 
  ylim(0, 1) +
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)

targets = c("hi1", "hi2")
fitted.w = fit_distr("memorable_unforgettable", numIters=200, alpha=4, eta=0.01, normalize=T)
# fitted.w = fitted.w / sum(fitted.w)
scalar.focused = data.full.extras[data.full.extras$scale == "memorable_unforgettable", ]
scalar.focused[scalar.focused$degree == "mid", "speaker.p"] = fitted.w
m = scalar.focused %>%
  do(run.full(., alpha = 4, addMid=T, normalize=T))
cor(m[m$degree %in% targets, ]$listener.p, m[m$degree %in% targets, ]$pred)

normalize(fitted.w)

qplot(stars, listener.p, col=degree, 
      data=m[m$degree %in% targets,],
      ylab="Posterior p(rating | word)") + 
  ylim(0, 1) +
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
```