---
title: "Model Comparisons"
author: "BPeloquin"
output:
  html_document:
    highlight: pygments
    theme: flatly
    toc: yes
    toc_depth: 2
---

```{r include=FALSE}
rm(list=ls())

library(ggplot2)
library(knitr)
library(rjson)
library(dplyr)
library(tidyr)
library(gridExtra)
```

# RSA functionality
```{r}
# speaker informativity
# ---------------------
speaker.lhd = function(d, alpha, cost = 0) {
  exp(alpha*(log(d) - cost))
}
# speaker likelihood
# ------------------
speaker.prob = function(rating, degree, m, alpha) {
  numerator = speaker.lhd(m[rating, degree], alpha)
  norm = 0
  for (i in 1:ncol(m)) {
    norm = norm + speaker.lhd(m[rating, i], alpha)
  }
  return(numerator / norm)
}
# non-normalized posterior
# -----------------------
nn.post = function(rating, degree, m, alpha, useprior) {
  prior = priors[rating, "prior.p"]
  speaker.prob(rating, degree, m, alpha) * prior
}
# normalized posterior
# --------------------
norm.post = function(rating, degree, m, alpha, useprior) {
  nn = nn.post(rating, degree, m, alpha, useprior)
  norm = 0
  for (i in 1:5) {
    # summation over all scale mates
    norm = norm + nn.post(i, degree, m, alpha, useprior)
  }
  return(nn / norm)
}  
```

# Run model functionality
```{r}
# run.partial()
# ------------
# Run RSA with model1 (entailment) and model2 (entailment + generic)
run.partial = function(d, alpha = 1, useprior = F, usenone = F) {
  mat = d %>%
    select(stars, degree, speaker.p) %>%
    spread(degree, speaker.p) %>%
    mutate(hi = hi / sum(hi), 
           low = low / sum(low)) %>%
    select(hi, low)
  
  if (usenone) {
    mat$none = c(1, 0, 0, 0, 0)
  } 
  
  d$pred = round(as.numeric(mapply(norm.post, d$stars, d$degree, 
                                    MoreArgs = list(m = mat, 
                                                    alpha = alpha, 
                                                    useprior = useprior))), 
                  digits=4)
  
  return(d)
} 

# run.full()
# ------------
# Run RSA with model3 (full) with alternatives
run.full = function(d, alpha = 1, useprior = F, usenone = F) {
  mat = d %>%
    select(stars, degree, speaker.p) %>%
    spread(degree, speaker.p) %>%
    mutate(hi1 = hi1 / sum(hi1),
           hi2 = hi2 / sum(hi2),
           low1 = low1 / sum(low1),
           low2 = low2 / sum(low2)) %>%
    select(hi1, hi2, low1, low2)
  
  if (usenone) {
    mat$none = c(1, 0, 0, 0, 0)
  } 
  
  d$pred = round(as.numeric(mapply(norm.post, d$stars, d$degree, 
                                    MoreArgs = list(m = mat, 
                                                    alpha = alpha, 
                                                    useprior = useprior))), 
                  digits=4)
  
  return(d)
} 
```

## Tune hyperparams
```{r}
# tune.alhpa()
# ------------
# d        --> data
# alphas   --> range of alphas to test
# type     --> full or partial model
# useprior --> use uniform prior
# usenone  --> use gener None
tune.alpha = function(d, alphas = seq(from=1, to=10), type="partial", useprior = T, usenone=F, compare.data=NULL) {
  # Tune best alphas
  fit = sapply(alphas, FUN=function(n) {
    if (type == "partial") {
      md = d %>%
        do(run.partial(., alpha = n, useprior = useprior, usenone = usenone))
    } else {
      md = d %>%
        do(run.full(., alpha = n, useprior = F, usenone = F))
    }
    # Fit to e6 data
    if (!is.null(compare.data)) {
      matched.items = which((md[, "scale"] != "some_all" &
                               (md[, "degree"] == "hi2" | md[, "degree"] == "hi1")) |
                              (md[, "scale"] == "some_all" &
                                 (md[, "degree"] == "hi1" | md[, "degree"] == "low1")))
      md = md[matched.items, ]
      md$degree = ifelse(md$degree == "hi1", "hi", "low")
      stopifnot("listener.p" %in% colnames(compare.data))
      md$listener.p = compare.data$listener.p
    }
    
    # MSE
    return(mean((md$pred - md$listener.p)^2))
  })  
  # get lowest MSE
  best.alpha = which(fit == min(fit))
  return(best.alpha)
}
```

# Model Comparisons

## Data for entailment models
```{r entailment_models_data, message=FALSE, warning=FALSE}
speaker = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L0_e8.csv")
listener = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L1_e6.csv")
# Uniform priors
priors = data.frame(stars = seq(1, 5), prior.p = rep(0.2, 5))
# Combine speaker / listener
data.partial = left_join(speaker, listener) %>%
  left_join(priors) %>%
  rowwise %>%
  select(scale, degree, stars, speaker.p, listener.p, prior.p) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p)) %>%
  group_by(scale)
```

## Data for full model
```{r full_model_data, message=FALSE, warning=FALSE}
speaker = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L0_e10.csv")
listener = read.csv("~/Desktop/Projects/scalar_implicature/models/model_data/L1_e11.csv")
data.full = left_join(speaker, listener) %>%
  left_join(priors) %>%
  rowwise %>%
  select(scale, degree, stars, speaker.p, listener.p, prior.p) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p)) %>%
  group_by(scale)
```

## Match items between studies (full and partial)
```{r}
matched.items = which((data.full[, "scale"] != "some_all" &
        (data.full[, "degree"] == "hi2" | data.full[, "degree"] == "hi1")) |
       (data.full[, "scale"] == "some_all" &
          (data.full[, "degree"] == "hi1" | data.full[, "degree"] == "low1")))
```


## Run comparisons
```{r run_comparisons, message=FALSE}
# Save performance output
performance.output = data.frame(model=rep(NA, 6), r2=rep(NA, 6))
alphas = rep(NA, 6)

# Model 1 - Entailment, no alpha tuning
# -------------------------------------
alphas[1] = 1
m1.noFit = data.partial %>%
  do(run.partial(., alpha = alphas[1], useprior = T, usenone = F))
performance.output[1, ] = c("M1_noFit", round(cor(m1.noFit$listener.p, m1.noFit$pred)^2, 5))

# Model 1 - Entailment, alpha tuning
# ----------------------------------
alphas[2] = tune.alpha(data.partial)
m1.fit = data.partial %>%
  do(run.partial(., alpha = alphas[2]))
performance.output[2, ] = c("M1_fit", round(cor(m1.fit$listener.p, m1.fit$pred)^2, 5))

# Model 2 - Entailment, generic "None", no alpha tuning
# -----------------------------------------------------
alphas[3] = 1
m2.noFit = data.partial %>%
  do(run.partial(., alpha = alphas[3], usenone = T))
performance.output[3, ] = c("M2_noFit", round(cor(m2.noFit$listener.p, m2.noFit$pred)^2, 5))

# Model 2 - Entailment, generic "None", alpha tuning
# --------------------------------------------------
alphas[4] = tune.alpha(data.partial, usenone = T)
m2.fit = data.partial %>%
  do(run.partial(., alpha = alphas[4], usenone = T))
performance.output[4, ] = c("M2_fit", round(cor(m2.fit$listener.p, m2.fit$pred)^2, 5))

# Model 3 - Full, no alpha tuning
# -------------------------------
alphas[5] = 1
m3.noFit = data.full %>%
  do(run.full(., alpha = alphas[5]))
m3.noFit.matched = m3.noFit[matched.items, ]
m3.noFit.matched$degree = ifelse(m3.noFit.matched$degree == "hi1", "hi", "low")
m3.noFit.matched = cbind(m3.noFit.matched, data.partial$listener.p)
# Check that everything lines up between partial and full
all(m3.noFit.matched$scale == data.partial$scale & m3.noFit.matched$degree == data.partial$degree)
colnames(m3.noFit.matched)[length(colnames(m3.noFit.matched))] = "e6.listener.p"
performance.output[5, ] = c("M3_noFit", round(cor(m3.noFit.matched$e6.listener.p, m3.noFit.matched$pred)^2, 5))

# Model 3 - Full, alpha tuning
# ----------------------------
alphas[6] = tune.alpha(data.full, type = "full", compare.data=data.partial)
m3.fit = data.full %>%
  do(run.full(., alpha = alphas[6]))
m3.fit.matched = m3.fit[matched.items, ]
m3.fit.matched$degree = ifelse(m3.fit.matched$degree == "hi1", "hi", "low")
m3.fit.matched = cbind(m3.fit.matched, data.partial$listener.p)
# Check that everything lines up between partial and full
all(m3.fit.matched$scale == data.partial$scale & m3.fit.matched$degree == data.partial$degree)
colnames(m3.fit.matched)[length(colnames(m3.fit.matched))] = "e6.listener.p"
performance.output[6, ] = c("M3_fit", round(cor(m3.fit.matched$e6.listener.p, m3.fit.matched$pred)^2, 5))
```

# Plots

Overall model performance: $r^2$. M1 and M2 use literal listener values from exp8. M2 includes a generic "None" defined in terms of stars (1: 1.0, 2:0.0, 3:0.0, 4:0.0, 5:0.0). M3 uses the full set of literal listener alternatives from exp10, however r^2 values and tuning reflects comparisons to exp6 pragmatic listener judgments (not exp11).
```{r overall_perf}
performance.output$model = factor(performance.output$model, levels=c(
  "M1_noFit", "M1_fit", "M2_noFit", "M2_fit", "M3_noFit", "M3_fit"
))
# Populate extra model info
performance.output$r2 = as.numeric(performance.output$r2)
performance.output$alphas = alphas
performance.output$alts = c(rep("entailment only", 2), rep("entailment + None", 2), rep("full", 2))
grid.table(performance.output)

qplot(data=performance.output, x=model, y=performance.output$r2,
      geom="bar", stat="identity",
      main = "Model performance", ylab="r^2", xlab="model") +
  geom_text(aes(label = round(r2, 3), y = r2 + 0.02), size = 3) +
  scale_y_continuous(limits=c(0, 1))
```

## Individual model performance
```{r individual_plots}
m1.noFit.G = qplot(stars, listener.p, col=degree, 
      data=m1.noFit,
      main=paste("M1, no fit\nalpha: ", alphas[1]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m1.noFit.G

m1.fit.G = qplot(stars, listener.p, col=degree, 
      data=m1.fit,
      main=paste("M1, fit\nalpha: ", alphas[2]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m1.fit.G

m2.noFit.G = qplot(stars, listener.p, col=degree, 
      data=m2.noFit,
      main=paste("M2, no fit\nalpha: ", alphas[3]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m2.noFit.G

m2.fit.G = qplot(stars, listener.p, col=degree, 
      data=m2.fit,
      main=paste("M2, fit\nalpha: ", alphas[4]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m2.fit.G

m3.matched.noFit.G = qplot(stars, e6.listener.p, col=degree, 
      data=m3.noFit.matched,
      main=paste("M3, no fit\nalpha: ", alphas[5]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m3.matched.noFit.G

m3.matched.fit.G = qplot(stars, e6.listener.p, col=degree, 
      data=m3.fit.matched,
      main=paste("M3, fit\nalpha: ", alphas[6]),
      ylab="Posterior p(rating | word)") + 
  facet_wrap(~scale) + 
  geom_line(aes(y = pred), lty = 4)
m3.matched.fit.G

# Full model fitted - color scale
m3.matched.fit.CorPlot = ggplot(data=m3.fit.matched, aes(y=pred, x=e6.listener.p)) +
  geom_point(aes(colour = scale)) +
  geom_smooth(method=lm) +
  ggtitle("Model performance: M3 fit") +
  labs(x = "human judgments", y="model prediction")
m3.matched.fit.CorPlot

# Full model fitted - color stars
m3.matched.fit.CorPlot = ggplot(data=m3.fit.matched, aes(y=pred, x=e6.listener.p)) +
  geom_point(aes(colour = stars)) +
  geom_smooth(method=lm) +
  ggtitle("Model performance: M3 fit") +
  labs(x = "human judgments", y="model prediction")
m3.matched.fit.CorPlot
```

# Other exploration
```{r}
bad.predictions = m3.fit.matched$pred > 0 &
                  m3.fit.matched$e6.listener.p == 0 &
                  (m3.fit.matched$pred - m3.fit.matched$e6.listener.p > 0.05)
```
