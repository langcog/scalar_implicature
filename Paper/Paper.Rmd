---
title: "Determining the alternatives for scalar implicature"
bibliography: library.bib
csl: apa6.csl
document-params: "10pt, letterpaper"

author-information: 
    \author{{\large \bf Benjamin Peloquin} \\ \texttt{bpeloqui@stanford.edu} \\ Department of Psychology \\ Stanford University
    \And {\large \bf Michael C. Frank} \\ \texttt{mcfrank@stanford.edu} \\ Department of Psychology \\ Stanford University}

abstract: 
    Successful communication regularly requires listeners to make pragmatic inferences - enrichments beyond the literal meaning of a speaker's utterance. For example, when interpreting a sentence such as "Alice ate some of the cookies," listeners routinely infer that Alice did not eat all of them. A Gricean account of this phenomena assumes the presence of alternatives (like "all of the cookies") with varying degrees of informativity, but it remains an open question precisely what these alternatives are. We collect empirical measurements of speaker and listener judgments and use these as inputs to a computational model of pragmatic inference. This approach allows us to test hypotheses about how well different sets of alternatives predict implicature performance across a range of different scales. Our findings suggest that comprehenders likely consider a much broader set of alternatives beyond those entailed by the initial description.  
    
keywords:
    "pragmatics; scalar implicature; bayesian modeling"
    
output: cogsci2016::cogsci_paper
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(fig.width=3, fig.height=3, fig.crop = F, fig.pos = "tb", fig.path='figs/',
                      echo=F, warning=F, cache=F, message=F, sanitize = T)
```

```{r, libraries}
library(dplyr)
library(ggplot2)
library(grid)
library(gridExtra)
library(jsonlite)
library(lme4)
library(lmerTest)
library(knitr)
library(png)
library(rjson)
library(tidyr)
library(xtable)
# library(langcog)
```


```{r helpers}
# lookupScalar()
# --------------
# degree :: scale degree ("hi", "low", "hi2" "h1", etc)
# scale  :: "some_all", "good_excellent"...
# exp    :: exp ide (e12, e8)
lookupScalar <- function(degree, scale, exp) {
  if (scale == "some_all") {
    if (exp == "e12") {
      if (degree == "hi1") return("all")
      if (degree == "hi2") return("most")
      if (degree == "mid") return("some")
      if (degree == "low1") return("little")
      if (degree == "low2") return("none")
    } else if (exp == "e10" | exp == "e11") {
      if (degree == "hi1") return("all")
      if (degree == "hi2") return("most")
      if (degree == "low1") return("some")
      if (degree == "low2") return("none")
    } else { ## must be exp 8
      if (degree == "hi") return("all")
      if (degree == "low") return("some")
    }
  # liked_loved
  } else if (scale == "liked_loved") {
    if (degree == "hi1" | degree == "hi") return("loved")
    if (degree == "hi2" | degree == "low") return("liked")
    if (degree == "mid") return("indifferent")
    if (degree == "low1") return("disliked")
    if (degree == "low2") return("hated")
  # good_excellent
  } else if (scale == "good_excellent") {
    if (degree == "hi1" | degree == "hi") return("excellent")
    if (degree == "hi2" | degree == "low") return("good")
    if (degree == "mid") return("okay")
    if (degree == "low1") return("bad")
    if (degree == "low2") return("horrible")
  # paltable_delicious
  } else if (scale == "palatable_delicious") {
    if (degree == "hi1" | degree == "hi") return("delicious")
    if (degree == "hi2" | degree == "low") return("palatable")
    if (degree == "mid") return("mediocre")
    if (degree == "low1") return("gross")
    if (degree == "low2") return("disgusting")
  # memorable_unforgettable
  } else if (scale == "memorable_unforgettable") {
    if (degree == "hi1" | degree == "hi") return("unforgettable")
    if (degree == "hi2" | degree == "low") return("memorable")
    if (degree == "mid") return("ordinary")
    if (degree == "low1") return("bland")
    if (degree == "low2") return("forgettable")
  }
  NA
}

normedVec <- function(vec) vec / sum(vec)
```

```{r, data}
# All scales studied
# ------------------
scales <- c("good_excellent",
            "liked_loved",
            "palatable_delicious",
            "memorable_unforgettable",
            "some_all")
# Experiment E9: Alternatives
# ---------------------------
alternativesData <- read.csv("../models/model_data/e9_alternativeCounts.csv",
                             stringsAsFactors = FALSE)
allModelRunData <- read.csv("../models/model_data/allModelRuns.csv",
                     stringsAsFactors = FALSE)

# Non-summarized data
# -------------------
allLiteralListener <- read.csv("../models/model_data/RawLiteralListenerCombined.csv")
allPragmaticListener <- read.csv("../models/model_data/RawPragmaticListenerCombined.csv")

# Model run data
# --------------
# Speakers ::
speakerE12 <- read.csv("../models/model_data/L0_e12.csv", stringsAsFactors = FALSE)
speakerE10 <- read.csv("../models/model_data/L0_e10.csv", stringsAsFactors = FALSE)
# speakerE8 <- read.csv("../models/model_data/L0_e8.csv", stringsAsFactors = FALSE) ## for nonCompliance data
# speakerGenericNone <- read.csv("../models/model_data/L0_e8_genericNone.csv", stringsAsFactors = FALSE) ## for genericNone data
speakerE8 <- read.csv("../models/model_data/L0_e8Compliance.csv", stringsAsFactors = FALSE)

# Listeners ::
listenerE6 <- read.csv("../models/model_data/L1_e6.csv", stringsAsFactors = FALSE)
listenerE11 <- read.csv("../models/model_data/L1_e11.csv", stringsAsFactors = FALSE)
```

# Introduction

How much of what we mean comes from the words that go unsaid? As listeners, our ability to make precise inferences about a speaker's intended meaning in context is  indispensable to successful communication. For example, listeners commonly enrich the meaning of the scalar item *some* to *some but not all* in sentences like "Alice ate some of the cookies" [@grice1975;@horn1984;@levinson2000]. These inferences, called _scalar implicatures_, have been an important test case for understanding pragmatic inferences more generally. A Gricean account of this phenomenon assumes listeners reason about the meaning the speaker intended by incorporating knowledge about a) alternative scalar items a speaker could have used (such as *all*) and b) the relative informativity of using such alternatives [@grice1975]. According to this account, a listener will infer that the speaker must have intended that Alice did not eat all the cookies because otherwise it would have been underinformative to use the descriptor *some* when the alternative *all* could have been used just as easily.

But what are the alternatives that should be considered in the implicature computation more generally? Under classic accounts, listeners consider only those words whose meanings entail the actual message [@horn1972], and these alternatives enter into conventionalized or semi-conventionalized scales [@levinson2000]. For example, because *all* entails *some*, and hence is a "stronger" meaning, *all* should be considered as an alternative to *some* in implicatures. Similar scales exist for non-quantifier scales, e.g. *loved* entails *liked* (and hence "I liked the movie" implicates that I didn't love it).

Recent empirical evidence has called into question whether entailment scales are all that is necessary for understanding scalar implicature. For example, @degen2015  demonstrated that the scalar item *some* was judged less appropriate when exact numbers were seen as viable alternatives. And in a different paradigm, @vantiel2014 found converging evidence that *some* was judged to be atypical for small quantities. These data provide indirect evidence about a broader set of alternatives: since *some* is logically true of sets with one or two members, these authors argued that the presence of salient alternatives (the words *one* and *two*, for example) reduced the felicity of *some* via a pragmatic inference. 

By formalizing pragmatic reasoning, computational models can help provide more direct evidence about the role that alternatives play. The "rational speech act" model (RSA) is one recent framework for understanding inferences about meaning in context [@frank2012;@goodman2013]. RSA models frame language understanding as a special case of social cognition, in which listeners and speakers reason recursively about one another's goals. In the case of scalar implicature, a listener makes a probabilistic inference about what the speaker's most likely communicative goal was, given that she picked the quantifier *some* rather than the stronger quantifier *all*. In turn, the speaker reasons about what message would best convey her intended meaning to the listener, given that he is reasoning in this way. This recursion is grounded in a "literal" listener who reasons only according to the basic truth-functional semantics of the language. 

@franke2014 used an RSA-style model to assess what alternatives a speaker would need to consider in order to produce the typicality/felicity ratings reported by @degen2015 and @vantiel2014. In order to do this, @franke2014's model assigned weights to a set of alternative numerical expressions. Surprisingly, along with weighting *one* highly (a conclusion that was supported by the empirical work), the best-fitting model assigned substantial weight to *none* as an alternative. This finding was especially surprising considering the emphasis of standard theories on scalar items that stand in entailment relationships with one another (e.g. *one* entails *some* even if it is not classically considered to be part of the scale).

In our current work, we pick up where these previous studies left off,  considering the set of alternatives for implicature using the RSA model. To gain empirical traction on this issue, however, we broaden the set of scales we consider. Our inspiration for this move comes from work by @vantiel2014b, who examined a phenomenon that they dubbed "scalar diversity," namely the substantial difference in the strength of scalar implicature across a variety of scalar pairs (e.g. *liked/loved,* or *palatable/delicious.*). Making use of some of this diversity allows us to investigate the ways that different alternative sets give rise to implicatures of different strengths across scales. 

We begin by presenting the computational framework we use throughout the paper. We next describe a series of experiments designed to measure both the literal semantics of a set of scalar items and comprehenders' pragmatic judgments for these same items. These experiments allow us to compare the effects of different alternative sets on our ability to model listeners' pragmatic judgments. To preview our results: we find that standard entailment alternatives do not allow us to fit participants' judgments, but that expanding the range of alternatives empirically (by asking participants to generate alternative messages) allows us to model listener judgments with high accuracy. 

# Modeling Implicature Using RSA

We begin by giving a brief presentation of the basic RSA model. This model simulates the judgments of a pragmatic listener who wants to infer a speaker's intended meaning $m$ from her utterance $u$. For simplicity, we present a version of this model in which there is only full recursion: that is, the pragmatic listener reasons about a pragmatic speaker, who in turn reasons about a "literal listener." We assume throughout that this computation takes place in a signaling game [@lewis1969] with a fixed set of possible meanings $m \in M$ and a fixed possible set of utterances $u \in U$, with both known to both participants. Our goal in this study is to determine what utterances fall in $U$.

In the standard RSA model, the pragmatic listener (denoted $L_1$), makes a Bayesian inference:

$$p_{L_1}(m \mid u) \propto p_{S_1} (u \mid m) p(m) \label{eq:rsa}$$

\noindent In other words, the probability of a particular meaning given an utterance is proportional to the speaker's probability of using that particular utterance to express that meaning, weighted by a prior over meanings. This prior represents the listener's _a priori_ expectations about plausible meanings, independent of the utterance. Because our experiments take place in a context in which listeners should have very little expectation about which meanings speakers want to convey, for simplicity we assume a uniform prior $p(m) \propto 1$. 

The pragmatic speaker in turn considers the probability that a literal listener would interpret her utterance correctly:

$$ p_{S_1}(u \mid m) \propto p_{L_0} (m \mid u)$$

\noindent where $L_0$ refers to a listener who only considers the truth-functional semantics of the utterance (that is, which meanings the utterance can refer to). 

This model of the pragmatic speaker (denoted $S_1$) is consistent with a speaker who chooses words to maximize the utility of an utterance in context [@frank2012], where utility is operationalized as the informativity of a particular utterance (surprisal) minus a cost:

$$p_{S_1}(u \mid m) \propto e^{-\alpha\[-log(p_{L_0}(m \mid u)) - C(u)\]}$$

\noindent where $C(u)$ is the cost of a particular utterance, $-log(p_{L_0})$ represents the _surprisal_ of the message for the literal listener (the information content of the utterance), and $\alpha$ is a parameter in a standard choice rule. If $\alpha=0$, speakers choose randomly and as $\alpha \rightarrow \infty$, they greedily choose the highest probability alternative. In our simulations below, we treat $\alpha$ as a free parameter and fit it to the data. 

To instantiate our signaling game with a tractable message set $M$, in our studies we adopt the world of restaurant reviews as our communication game. We assume that speakers and listeners are trying to communicate the number of stars in an online restaurant review (where $m \in \{1, 2, 3, 4, 5\}$). We then use experiments to measure three components of the model. First, to measure literal semantics ${p_{L_0} (m \mid u)}$ (we ask experiment participants to judge whether a message is compatible with a particular meaning (Experiment 1). Second, to generate a set of plausible alternative messages in $U$, we elicit alternatives directly (Experiment 2). Lastly, to obtain human $L_1$ pragmatic judgments, we ask participants to interpret a speaker's utterances (Experiment 3). 

# Experiment 1: Literal listener task

```{r allScalesTable, fig.pos = "t", fig.width=3.5, fig.height=2.5, fig.cap = "Stimuli for Experiments 1, 2, and 3"}

# Scale stimuli
grid::grid.raster(png::readPNG("figs/alternativeSets.png"))
```

```{r stimuli_exp1, fig.env = "figure*", fig.pos = "t", fig.width=6, fig.height=4, fig.align='center', set.cap.width=TRUE, num.cols.cap=2, fig.cap = "(Left) A trial from Experiment 1 (literal listener) with the target scalar \`liked.\' (Right) A trial from Experiment 3 (pragmatic listener) with the target scalar \`liked.\'"}

# Screen shots of literal listener and pragmatic listener conditions
grid::grid.raster(png::readPNG("figs/exp1_stimuli.png"))
```

Experiment 1 was conducted to approximate literal listener semantic distributions $p_{L_0}(m \mid u)$ for five pairs of scalar items taken from @vantiel2014. We include three conditions in Experiment 1, corresponding to the sets of alternatives within a scale that participants were presented with: two alternatives ("entailment"), four alternatives, and five alternatives. The two alternatives condition makes a test of the hypothesis that the two members of the classic Horn (entailment) scale [@horn1972] are the only alternatives necessary to predict the strength of listeners' pragmatic inference. The four and five alternatives conditions then add successively more alternatives to test whether including a larger number of alternatives will increase model fit.\footnote{Note that alternatives in the four and five alternatives conditions were chosen on the basis of Experiment 2, which was run chronologically after the two-alternative condition; all literal listener experiments are grouped together for simplicity in reporting.} A secondary goal of Experiment 1 is to test whether the set of alternatives queried during literal semantic elicitation impacts compatibility judgments. (If it does we should see differences in compatibility judgments for shared items between experiments.) 

## Methods

### Participants
```{r}
exp1a <- subset(allLiteralListener, exp == "e8")
exp1b <- subset(allLiteralListener, exp == "e10")
exp1c <- subset(allLiteralListener, exp == "e12")
```

Conditions were run sequentially. In each condition we recruited 30 participants from Amazon Mechanical Turk (AMT). In the two alternative condition, `r 30 - length(unique(exp1a$workerid))` participants were excluded for either failing to pass two training trails or were not native English speakers, leaving a total sample of `r length(unique(exp1a$workerid))` participants.\footnote{The majority of respondent data excluded from the two alternative condition was caused by failure to pass training trials. We believe the task may have been too difficult for most respondents and made adjustments to the training trials in later conditions.} In the four alternative condition,  `r 30 - length(unique(exp1b$workerid))` participants were excluded for either failing to pass two training trials or were not native English speakers, leaving a total sample of `r length(unique(exp1b$workerid))` participants. In the five alternative condition, `r 30 - length(unique(exp1c$workerid))` participants were excluded for either failing to pass two training trials or were not native English speakers, leaving a total sample of `r length(unique(exp1c$workerid))`.

### Design and procedure

Figure \ref{fig:stimuli_exp1}, left, shows the experimental setup. Participants were presented with a target scalar item and a star rating (1--5 stars) and asked to judge the compatibility of the scalar item and star rating. Compatibility was assessed through a binary "yes/no" response to a question of the form, "Do you think that the person thought the food was \____?" where a target scalar was presented in the blank. Each participant saw all scalar item and star rating combinations for their particular condition, in a random order.

The two alternatives condition included only the scalar pairs from @vantiel2014. The four alternatives condition included the two scalar items plus the top two alternatives generated for each scalar family by participants in Experiment 2. The five alternatives condition included the four previous items plus one more neutral item chosen from those alternatives generated in Experiment 2. See Figure \ref{fig:allScalesTable} for the complete list of alternatives used in each condition. 

## Results and Discussion

```{r exp1Plots, fig.env = "figure*", fig.pos = "t", fig.width=6.5, fig.height=5, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Literal listener judgments from Experiments 1a,b,c. Proportion of participants indicating compatibility (answering 'yes') is shown on the vertical axis, with the horizontal axis showing number of stars on which the utterance was judged. Rows are grouped by scale and items within rows are ordered by valence. Colors indicate the specific experiment (1a,b,c) with experiments including different numbers of items."}

speakers <- bind_rows(speakerE12 %>% mutate(expt = "e12"), 
                      speakerE10 %>% mutate(expt = "e10"), 
                      speakerE8 %>% mutate(expt = "e8"))
speakers$scale <- as.factor(speakers$scale)

speakers$word <- as.factor(with(speakers, mapply(lookupScalar, degree, scale, expt)))
speakers$wordPlots <- factor(speakers$word, levels=c("horrible", "bad", "okay", "good", "excellent",
  "hated", "disliked", "indifferent", "liked", "loved",
  "forgettable", "bland", "ordinary", "memorable", "unforgettable",
  "disgusting", "gross", "mediocre", "palatable", "delicious",
  "none", "little", "some", "most", "all"))

# for plotting
speakers$condition <- ifelse(speakers$expt == "e8", "two alts",
                                 ifelse(speakers$expt == "e10", 
                                        "four alts", "five alts"))
ggplot(speakers, 
       aes(x = stars, y = speaker.p, col = condition)) + 
  geom_line() + 
  # facet_grid(.~ scale) + 
  facet_wrap(~wordPlots) +
  guides(colour = guide_legend(direction = "horizontal", nrow = 1, 
                               title = "Condition")) + 
  xlab("star rating") +
  ylab("proportion compatible ('yes' responses)") + 
  # ggtitle("Scalar item literal semantic compatibility\nwith star ratings") +
  # scale_colour_solarized() + 
  theme_bw() +
  theme(strip.text.x = element_text(size = 6),
        legend.position = "bottom",
        legend.margin = unit(.02,"in"),
        legend.key = element_blank(), 
        legend.key.size = unit(.1, "in"),
        legend.title = element_text(size = 10),
        axis.text.x = element_text(size = 7),
        axis.text.y = element_text(size = 6))
```

Figure \ref{fig:exp2Plots} plots literal listener $p_{L0}(m|u)$ scalar item distributions for the three conditions. Each row shows a unique scalar family with items ordered horizontally by valence. Several trends are visible. First, in each scale, the alternatives spanned the star scale, such that there were alternatives that were highly compatible with both the lowest and highest numbers of stars. Second, there was clear variability between scalar families. For example, compatibility judgments for the top items in the *memorable / unforgettable* scale were more similar than those for *good / excellent* or *liked / loved*. Finally, there was substantial consistency in ratings for items that were repeated across experiments, suggesting that this paradigm elicited stable judgments from participants. 

```{r literalListener_ME_models, echo = FALSE, warning = FALSE, message = FALSE}
# fix contrasts so we're comparing most disparate experimetns (e8 and e12)
contrasts(allLiteralListener$exp) <- cbind("e10" = c(1, 0, 0), "e8" = c(0, 0, 1))

# glmer WITHOUT experiment and with random effect for subjects
glm1 <- glmer(judgment ~ scale + stars + (1 | word) + (1 | workerid),
              family=  "binomial", data = allLiteralListener)
# glmer WITH experiment and with random effect for subjects
glm2 <- glmer(judgment ~ exp + scale +  stars + (1 | word) +  (1 | workerid),
              family = "binomial", data = allLiteralListener)
sumGlm2 <- summary(glm2)
anovaResults <- anova(glm1, glm2)
```
To test our secondary hypothesis, that the different sets of scalar items might result in differences in compatibility judgments between experiments, we ran a mixed effects model. We regressed compatibility judgments on scale, number of stars and condition, with subject and word level random effects, which was the maximal structure that converged. Results indicate no significant differences between Experiment 1a and 1c, $b =$`r round(sumGlm2$coefficients[3], 2)`, $Z =$ `r round(sumGlm2$coefficients[19], 2)`, $p =$ `r round(sumGlm2$coefficients[27], 2)` or between 1b and 1c, $b =$ `r round(sumGlm2$coefficients[2], 2)`, $Z =$ `r round(sumGlm2$coefficients[18], 2)` $p =$ `r round(sumGlm2$coefficients[26], 2)` and the addition of condition as a predictor did not significantly improve model fit when compared to a model without the experiment variable using ANOVA $\chi^2($ `r anovaResults$"Chi Df"[2]` $) =$ `r round(anovaResults$Chisq[2], 2)`, $p =$ `r round(anovaResults$"Pr(>Chisq)"[2], 2)`.

# Experiment 2: Alternative Elicitation

To elicit empirical alternatives for the scales we used in Experiment 1, we adopted a modified cloze task inspired by Experiment 3 of @vantiel2014.

## Methods

### Participants

We recruited 30 workers on AMT. All participants were native English speakers and naive to the purpose of the experiment.

### Design and procedure

Participants were presented a target scalar item from our original entailment set (see Figure \ref{fig:allScalesTable}) embedded in a sentence such as, "In a recent restaurant review someone said they thought they the food was \____," with a target scalar presented in the blank. Participants were then asked to generate plausible alternatives by responding to the question, "If they'd felt differently about the food, what other words could they have used instead of \____?" They were prompted to generate three unique alternatives.

## Results and Discussion

```{r exp2_altsPlot_likedLoved, fig.pos = "t", fig.align='center', set.cap.width=T, fig.height=2, num.cols.cap=2, fig.cap = "Combined counts for participant-generated alternatives for the "liked \ loved" scale in Experiment 2."}

# Subset alternatives for demo
plotData <- alternativesData %>%
  filter(scale == "liked_loved", alt != "liked", alt != "loved") %>%
  mutate(prop = n / sum(n))
  
# plotData <- head(plotData[order(plotData$n, decreasing = TRUE),], n = 24)
ggplot(data = plotData, aes(x = reorder(alt, -n), y = n)) +
  geom_bar(stat="identity") + 
  ylim(0, 52) + 
  labs(x = "alternatives", y = "counts") +
  theme_bw() +
  theme(strip.text.x = element_text(size = 8),
        legend.position = "bottom",
        legend.margin = unit(.02,"in"),
        legend.key = element_blank(), 
        legend.key.size = unit(.1, "in"),
        axis.text.x = element_text(angle = 90, hjust = 1, vjust = .5, size = 6),
        axis.text.y = element_text(size = 10),
        axis.title.x = element_text(size = 10),
        axis.title.y = element_text(size = 10))
```


```{r exp2Plots, fig.env = "figure*", fig.pos = "t", fig.width=6.5, fig.height=2.5, fig.align='center', set.cap.width=T, num.cols.cap=2, fig.cap = "Pragmatic listener judgements for scalar items. Proportion of participants generating a star rating is shown on the vertical axis, with the horizontal axis showing number of stars on which the utterance was judged. Line type denotes condition, and colors indicate the particular scalar items. Each panel shows one scalar pair, with only entailment items (two alternatives condition) shown here for simplicity."}
# Get zero's in data frame
baseDf <- data.frame(degree = rep(c("hi", "low", "hi1", "hi2", "low1", "low2"), 25),
                     scale = c(rep("good_excellent", 6 * 5),
                               rep("liked_loved", 6 * 5),
                               rep("memorable_unforgettable", 6 * 5),
                               rep("palatable_delicious", 6 * 5),
                               rep("some_all", 6 * 5)),
                     stars = rep(1:5, 30),
                     combine = rep(TRUE, 150))
listeners <- bind_rows(listenerE6 %>% mutate(expt = "e6"), 
                      listenerE11 %>% mutate(expt = "e11"))
listeners <- left_join(baseDf, listeners) %>%
  mutate(listener.p = ifelse(is.na(listener.p), 0, listener.p),
         expt = ifelse(degree == "hi" | degree == "low", "e6", "e11"))
listeners$word <- as.factor(with(listeners, mapply(lookupScalar, degree, scale, expt)))
listeners$plotThis <- with(listeners, ifelse(scale == "some_all" &
                               (degree == "hi" | degree == "low" | degree == "hi1" | degree == "low1"),
                               TRUE, ifelse(scale != "some_all" & (
                                 degree == "hi" | degree == "low" | degree == "hi1" | degree == "hi2"),
                                 TRUE, FALSE)))
listeners$`Scalar type` <- with(listeners, ifelse(degree == "hi" | degree == "hi1", "stronger", "weaker"))
listeners$Condition <- with(listeners, ifelse(expt == "e6", "two alts", "four alts"))
                               
# Plot pragmatic listener data here
ggplot(subset(listeners, plotThis == TRUE),
       aes(x = stars, y = listener.p, col = `Scalar type`, lty = Condition)) + 
  geom_line() + 
  facet_grid(.~ scale) + 
  # ggtitle("Pragmatic listener judgments") +
  ylab("percentage selecting") +
  xlab("star rating") +
  theme_bw() +
  theme(strip.text.x = element_text(size = 6),
        legend.position = "bottom",
        legend.margin = unit(.02,"in"),
        legend.key = element_blank(), 
        legend.key.size = unit(.1, "in"),
        legend.title = element_text(size = 10),
        legend.text = element_text(size = 8),
        axis.title.x = element_text(size = 8.5),
        axis.title.y = element_text(size = 10),
        axis.text.x = element_text(size = 10),
        axis.text.y = element_text(size = 10))
```

Figure \ref{fig:exp2_altsPlot_likedLoved} shows an example alternative set for the scalar items *liked* and *loved* (combined). Alternative distributions for the other scalar pairs (e.g.. *good/excellent*, *memorable/unforgettable*) were similarly long-tailed.

# Experiment 3: Pragmatic Listener

Experiment 3 was conducted to measure pragmatic judgments---$p_{L_1}(m \mid u)$ in Equation \ref{eq:rsa}. As in Experiment 1, we include several conditions to test inferences in the presence of different alternative sets. In the two alternatives condition, participants made judgments for items included in the entailment scales. In the four alternatives condition, participants made judgments for the entailment items and also the top two alternatives elicited for each scale in Experiment 2. Including two conditions with differing alternatives allowed us to rule out the potential effects of having a larger set of alternatives during the pragmatic judgment elicitation and also provided two sets of human judgments to compare with model predictions.\footnote{Note that alternatives in the four alternatives condition were chosen on the basis of Experiment 2, which was run chronologically after the two alternatives condition; both pragmatic listener experiments are grouped together for simplicity in reporting.} 

## Participants

```{r, allPragStudySubsets, echo = FALSE, messgae = FALSE, warning = FALSE}
exp3a <- subset(allPragmaticListener, exp == "e6")
exp3b <- subset(allPragmaticListener, exp == "e11")
```

We recruited 100 participants from AMT, 50 for each condition. Data for `r 50 - length(unique(exp3a$workerid))` participants was excluded from the two alternatives condition after participants either failed to pass two training trials or were non-native English speakers, leaving a total sample of `r length(unique(exp3a$workerid))` participants. In the four alternative condition, data from `r 50 - length(unique(exp3b$workerid))` participants was excluded after participants either failed to pass two training trials or were not native English speakers, leaving `r length(unique(exp3b$workerid))` participants.

## Procedure

Participants were presented with a one-sentence prompt containing a target scalar item such as "Someone said they thought the food was \_____." Participants were then asked to generate a star rating representing the rating they thought the reviewer likely gave. Each participant was presented with all scalar items in a random order. The experimental setup is shown in Figure \ref{fig:stimuli_exp1}, right. 

## Results and Discussion
```{r pragmaticListener_ME_models, echo = FALSE, warning = FALSE, message = FALSE}
# glmer WITHOUT experiment and with random effect for subjects
glm1 <- lmer(stars ~ scale + (1 | word) + (1 | workerid), data = allPragmaticListener)
# glmer WITH experiment and with random effect for subjects
glm2 <- lmer(stars ~ exp + scale + (1 | word) +  (1 | workerid), data = allPragmaticListener)
sumGlm2 <- summary(glm2)
anovaResults <- anova(glm1, glm2)
```

Figure \ref{fig:exp2Plots} plots pragmatic listener judgments distributions for "weak" / "strong" scalar pairs (e.g. *good*/*excellent*). We include only the original entailment pairs in this figure for simplicity. Several trends are visible. First, in each scale participants generated implicatures. They were substantially  less likely to assign high star ratings to weaker scalar terms, despite the literal semantic compatibility of those terms with those states shown in Experiment 1. Second, the size of the difference between strong and weak scalar items varied considerably across scales, consistent with previous work [@vantiel2014].

To test our secondary hypothesis, that the different sets of scalar items in the two conditions might result in differences in pragmatic judgments for the same items, we fit a mixed effects model. We regressed pragmatic judgments on scale and condition with subject- and word-level random effects, which was the maximal structure that converged. There were no significant differences between conditions, $b =$ `r round(sumGlm2$coefficients[2], 2)`, $t($ `r round(sumGlm2$coefficients[14], 0)` $) =$ `r round(sumGlm2$coefficients[20], 2)`, $p =$ `r round(sumGlm2$coefficients[26], 2)` and the addition of the condition predictor did not significantly improve model fit when compared to a model without that variable $\chi^2($ `r anovaResults$"Chi Df"[2]` $) =$ `r round(anovaResults$Chisq[2], 2)`, $p =$ `r round(anovaResults$"Pr(>Chisq)"[2], 2)`.

# Model

Using literal listener data from Experiment 1, we conducted a set of simulations with the RSA model. Each simulation kept the model constant, fitting the choice parameter $\alpha$ as a free parameter, but used a set of alternatives to specify the scale over which predictions were computed. We considered four different alternative sets, with empirical measurements corresponding to those shown in Table \ref{fig:allScalesTable): 1) the two alternatives in the classic entailment scales, 2) those two alternatives with the addition of a generic negative alternative, 3) the expanded set of four alternatives tested in Experiment 1, and 4) the expanded set of five alternatives tested in Experiment 1. 

Model fit with human judgments was significantly improved by the inclusion of alternatives beyond the entailment items (Table \ref{fig:xtable}). The two alternatives model contained only entailment items, which, under classic accounts, should be sufficient to generate implicature, but fit to data was quite poor with these items. The addition of a generic negative element produced some gains in performance, but much higher performance was found when we included four and five alternatives, with the alternatives derived empirically for the specific scale we used. An example fit for the five-alternative model is shown in Figure \ref{fig:fiveAltsScatter}. 

```{r xtable, results = "asis", fig.env = "figure*", fig.pos = "t", fig.width=6, fig.height=4, fig.align='center', set.cap.width=T, num.cols.cap=2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "The left panel shows improved model fit as scale representations are enriched with more scalar items. Correlations are coputed using pragmatic judgment data from Experiments 1b and 3b. The right panel plots model predictions using full symmetric scales versus human judgments from Experiment 3b."}
# Tuned model run data
plotData <- allModelRunData %>%
  mutate(study = ifelse(numAlts == 5, "Five alts", 
                        ifelse(numAlts == 4, "Four alts",
                               ifelse(numAlts == 3, "Two alts + generic negative ", "Two alts")))) %>%
  gather(Experiment, cors, c(corE6, corE11)) %>%
  mutate(Experiment = ifelse(Experiment == "corE6", "1b", "3b"))

tableCors <- plotData  %>%
  select(cors, study, Experiment, alphas) %>%
  unique() %>%
  mutate(Experiment = ifelse(Experiment == "1b", "Pragmatic_Listener_exp1", "Pragmatic_Listener_exp2")) %>%
  spread(Experiment, cors)
  
# Plot model performance table
names(tableCors) <- c("Model", "$\\alpha$", "Exp. 3a", "Exp. 3b")
tableCors <- tableCors[c(3, 4, 2, 1),]
tab <- xtable(tableCors, caption = "Model performance with fitted alpha levels. Model fit assessed through correlation with human judgments in our two Pragmatic listener experiments (3a,b)")
print(tab, type="latex", floating = TRUE, include.rownames = FALSE, comment = FALSE,
      sanitize.colnames.function = identity)

# Correlation bar chart
# corBarChart <- ggplot(plotData, aes(x=study, y=cors, fill = Experiment)) + 
#   geom_bar(stat="identity", position="dodge") +
#   ylab("Correlation") +
#   xlab("Model") +
#   ylim(0, 1) + 
#   ggtitle("Model fit correlation coefficients") +
#   guides(colour=FALSE) +
#   geom_text(aes(x = study, y=as.numeric(cors) + 0.025, label = round(cors, 2), col=(Experiment)))

# modifiedPlotData <- plotData %>%
#   gather(pragStudy, pragChoice, c(9,11))
# fullRepresentationFit <- qplot(stars, pragChoice, col=degree, shape = pragStudy,
#       data=subset(modifiedPlotData, study == "Full")) +
#   ylim(0, 1) +
#   facet_wrap(~scale) + 
#   geom_line(aes(y = preds), lty = 4)
# fullRepresentationFit
# gridExtra::grid.arrange(corBarChart, symmetricCorPlot, ncol = 2, widths = c(2.5,3))
```


```{r fiveAltsScatter, fig.env = "figure*", fig.pos = "t", fig.width=4.5, fig.height=2, fig.align='center', set.cap.width=T, num.cols.cap=2, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Judgments from Experiment 3, four alternatives condition, are plotted against model predictions. Colors show the star rating for individual judgments."}
# Correlation scatter plot
ggplot(subset(plotData, study == "Five alts"), aes(x = listener.p2, y = preds, col=stars)) + 
  geom_point() +
  xlab("model predictions") +
  ylab("human judgments (Exp. 3b)") +
  stat_smooth(method = "lm") +
  theme_bw() + 
  theme(strip.text.x = element_text(size = 8),
#         legend.position = "bottom",
#         legend.margin = unit(.02,"in"),
#         legend.key = element_blank(), 
#         legend.key.size = unit(.1, "in"),
        axis.title.x = element_text(size = 8.5),
        axis.title.y = element_text(size = 8.5)) 
  # ggtitle("Model fit\nFive alternatives")

```

# General Discussion

Pragmatic inference requires reasoning about alternatives. The fundamental pragmatic computation is counterfactual: "if she had meant X, she would have said Y, but she didn't." Yet the nature of these alternatives has been controversial. For a few well-studied scales, a small set of logically-determined alternatives has been claimed to be all that is necessary [@horn1972]. For other, contextually-determined inferences, the issue of alternatives has been considered relatively intractable in terms of formal inquiry [@sperber1995]. 

In our current work, we used the rational speech act framework to investigate the set of alternatives that best allowed the model to predict pragmatic judgments across a series of different scales. We found that the best predictions came when a range of scale-dependent negative and neutral alternatives were added to the implicature computation, suggesting the importance of considering non-entailment alternatives. This work builds on previous investigations, reinforcing the claim that negative alternatives are critical for understanding implicature [@franke2014], and replicates and extends findings that different lexical scales produce strikingly different patterns of inference [@vantiel2015]. 

While improvements in model fit were substantial as we moved from two to four alternatives, we saw only a minor increase in fit from the move to five alternatives. One possible explanation is that alternatives are differentially salient in context, and moving to larger sets would consider weighting the alternatives differentially [as @franke2014 did]. Preliminary simulations using weightings derived from Experiment 2 provide some support for this idea but would require further empirical work for confirmation. 

The precise set of alternatives present during implicature is likely to be domain dependent. Our current empirical paradigm elicited literal semantics, pragmatic judgments, and plausible alternatives all within the restricted domain of restaurant reviews. Our measurements might have differed substantially if we had instead grounded our ratings in a different context. Future investigations should probe the context-specificity of the weight and availability of particular alternative sets. 

More broadly, considering the context- and domain-specificity of alternative sets may provide a way to unite what @grice1975 called "generalized" (cross-context) and "particularized" (context-dependent) implicatures. Rather than being grounded in a firm distinction, we may find that these categories are simply a reflection of the effects of context on a constantly-shifting set of pragmatic alternatives. 

# Acknowledgements

Thanks to NSF BCS #1456077 for support, and thanks to Michael Franke, Judith Degen, and Noah Goodman for valuable discussion.

# References 

\setlength{\parindent}{-0.1in} 
\setlength{\leftskip}{0.125in}
\noindent
